{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Familiarization Task\n",
    "\n",
    "&emsp;Load and understand data<br/>\n",
    "&emsp;&emsp;What kinds of signals are there?<br/>\n",
    "&emsp;&emsp;Are the signals correlated? Do they show cyclic behavior<br/>\n",
    "&emsp;&emsp;Is predicting the next value in a series easy or hard? Use any method from class<br/>\n",
    "&emsp;&emsp;Visualize correlation and the performance of prediction<br/>\n",
    "\n",
    "## 1.First step is to visualize data\n",
    "\n",
    "### Meaning of columns in dataframe<br/>\n",
    "&emsp;T: water tanks<br/>\n",
    "&emsp;&emsp;L_T: water level in tanks<br/>\n",
    "&emsp;PU: pumps<br/>\n",
    "&emsp;&emsp;F_PU: amount of water flows through pipe<br/>\n",
    "&emsp;&emsp;S_PU: whether the pipe is on(1) or off(0)<br/>\n",
    "&emsp;V: valves<br/>\n",
    "&emsp;&emsp;F_V: amount of water flows through valve<br/>\n",
    "&emsp;&emsp;S_V: whether the valve is on(1) or off(0)<br/>\n",
    "&emsp;J: junctions<br/>\n",
    "&emsp;&emsp;P_J: pressure of junction<br/>\n",
    "&emsp;ATT_FLAG: label indicating attacks<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform convertions and inspect the shape of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=pd.read_csv('BATADAL_dataset03.csv',skipinitialspace=True)\n",
    "#convert date time to pandas can read\n",
    "trainingData_1['DATETIME']=pd.to_datetime(trainingData_1['DATETIME'])\n",
    "print(f'columns of data1: {trainingData_1.columns}')\n",
    "#inspect the shape of Data\n",
    "print(f'Data Frame 1 shape: {trainingData_1.shape}')\n",
    "length_Data1=len(trainingData_1)\n",
    "print(f\"Data Frame 1 length: {((trainingData_1['DATETIME'][length_Data1-1]-trainingData_1['DATETIME'][0]).total_seconds()/(3600*24))} days\")\n",
    "trainingData_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect dataset2\n",
    "trainingData_2=pd.read_csv('BATADAL_dataset04.csv',skipinitialspace=True)\n",
    "trainingData_2['DATETIME']=pd.to_datetime(trainingData_2['DATETIME'])\n",
    "print(f'columns of data2: {trainingData_2.columns}')\n",
    "print(f'Data Frame 2 shape: {trainingData_2.shape}')\n",
    "length_Data2=len(trainingData_2)\n",
    "print(f\"Data Frame 2 length: {(trainingData_2['DATETIME'][length_Data2-1]-trainingData_2['DATETIME'][0]).total_seconds()/(3600*24)} days\")\n",
    "trainingData_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect test data\n",
    "testData=pd.read_csv('BATADAL_test_dataset.csv',skipinitialspace=True)\n",
    "testData['DATETIME']=pd.to_datetime(testData['DATETIME'])\n",
    "print(f'columns of data2: {testData.columns}')\n",
    "print(f'Data Frame test shape: {testData.shape}')\n",
    "length_Test=len(testData)\n",
    "print(f\"Data Frame test length: {(testData['DATETIME'][length_Test-1]-testData['DATETIME'][0]).total_seconds()/(3600*24)} days\")\n",
    "testData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.show time series plot and correlation matrix\n",
    "&emsp;Before doing that, all columns of data need to be preprocessed.<br/>\n",
    "&emsp;&emsp;Normalization is used so as to compare different signals.<br/>\n",
    "&emsp;&emsp;Non-attack in column 'ATT_FLAG' of trainingData_2 is mapped to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize dataset 1\n",
    "location=[]\n",
    "normalized_1=trainingData_1.copy()\n",
    "col=normalized_1.loc[:,~normalized_1.columns.isin(['ATT_FLAG','DATETIME'])].columns\n",
    "for i in col:\n",
    "    if normalized_1[i].std()==0:\n",
    "        location.append(i)\n",
    "normalized_1.loc[:,~normalized_1.columns.isin(['ATT_FLAG','DATETIME']+location)]=(normalized_1.loc[:,~normalized_1.columns.isin(['ATT_FLAG','DATETIME']+location)]-normalized_1.loc[:,~normalized_1.columns.isin(['ATT_FLAG','DATETIME']+location)].mean())/normalized_1.loc[:,~normalized_1.columns.isin(['ATT_FLAG','DATETIME']+location)].std()\n",
    "normalized_1.loc[:,location]=(normalized_1.loc[:,location]-normalized_1.loc[:,location].mean())\n",
    "normalized_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize dataset 2\n",
    "location=[]\n",
    "normalized_2=trainingData_2.copy()\n",
    "#map Non-attack in column 'ATT_FLAG' to 0\n",
    "normalized_2.loc[normalized_2['ATT_FLAG']==-999,'ATT_FLAG']=0\n",
    "col=normalized_2.loc[:,~normalized_2.columns.isin(['ATT_FLAG','DATETIME'])].columns\n",
    "for i in col:\n",
    "    if normalized_2[i].std()==0:\n",
    "        location.append(i)\n",
    "normalized_2.loc[:,~normalized_2.columns.isin(['ATT_FLAG','DATETIME']+location)]=(normalized_2.loc[:,~normalized_2.columns.isin(['ATT_FLAG','DATETIME']+location)]-normalized_2.loc[:,~normalized_2.columns.isin(['ATT_FLAG','DATETIME']+location)].mean())/normalized_2.loc[:,~normalized_2.columns.isin(['ATT_FLAG','DATETIME']+location)].std()\n",
    "normalized_2.loc[:,location]=(normalized_2.loc[:,location]-normalized_2.loc[:,location].mean())\n",
    "normalized_2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize testset\n",
    "location=[]\n",
    "normalized_t=testData.copy()\n",
    "col=normalized_t.loc[:,~normalized_t.columns.isin(['DATETIME'])].columns\n",
    "for i in col:\n",
    "    if normalized_t[i].std()==0:\n",
    "        location.append(i)\n",
    "normalized_t.loc[:,~normalized_t.columns.isin(['DATETIME']+location)]=(normalized_t.loc[:,~normalized_t.columns.isin(['DATETIME']+location)]-normalized_t.loc[:,~normalized_t.columns.isin(['DATETIME']+location)].mean())/normalized_t.loc[:,~normalized_t.columns.isin(['DATETIME']+location)].std()\n",
    "normalized_t.loc[:,location]=(normalized_t.loc[:,location]-normalized_t.loc[:,location].mean())\n",
    "normalized_t.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series plot, we only plot  in the 0-500 contiunous time slice\n",
    "print('Images for trainingData 1')\n",
    "col=normalized_1.columns.tolist()\n",
    "col.remove('DATETIME')\n",
    "col.remove('ATT_FLAG')\n",
    "for i in col:\n",
    "    normalized_1.loc[0:500,i].plot(figsize=(20,10))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Images for trainingData_2')\n",
    "col=normalized_2.columns.tolist()\n",
    "col.remove('DATETIME')\n",
    "col.remove('ATT_FLAG')\n",
    "for i in col:\n",
    "    normalized_2.loc[0:500,i].plot(figsize=(20,10),color='red')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Images for testData')\n",
    "col=normalized_t.columns.tolist()\n",
    "col.remove('DATETIME')\n",
    "for i in col:\n",
    "    normalized_t.loc[0:500,i].plot(figsize=(20,10),color='green')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show corrleation \n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "corr = normalized_1.corr()\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values,yticklabels=corr.columns.values,linewidth=0.5)\n",
    "plt.figure(figsize=(15, 15))\n",
    "corr = normalized_2.corr()\n",
    "sns.heatmap(corr,cmap='winter', xticklabels=corr.columns.values,yticklabels=corr.columns.values,linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calclate correlation\n",
    "def correlationCal(signal):\n",
    "    correlation=signal.corr()\n",
    "    for i in correlation.columns.tolist():\n",
    "        list=correlation.columns.tolist()\n",
    "        list.remove(i)\n",
    "        tmp=[abs(x) for x in (correlation[i].tolist())]\n",
    "        del tmp[correlation.columns.tolist().index(i)]\n",
    "        tmp2=(correlation[i].tolist())\n",
    "        del tmp2[correlation.columns.tolist().index(i)]\n",
    "        correlatedCol=[]\n",
    "        for j in range(len(list)):\n",
    "            if tmp[j]>0.7:\n",
    "                correlatedCol.append([list[j],tmp2[j]])\n",
    "        if len(correlatedCol)==0:\n",
    "            index=tmp.index(max(tmp))\n",
    "            correlatedCol.append([list[index],tmp2[index]])\n",
    "        print(f'most correlated items of {i} are :')\n",
    "        print(correlatedCol)\n",
    "        #plots of correlated signals\n",
    "        for j in correlatedCol:\n",
    "            signal[j[0]][0:300].plot(figsize=(20,10))\n",
    "        signal[i][0:300].plot(figsize=(20,10))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return\n",
    "\n",
    "correlationCal(normalized_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlationCal(normalized_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Make prediction using sliding window and linear regression\n",
    "\n",
    "To Do: Other options: Regression Trees, Probabilistic Models, Polynomial Regression\n",
    "\n",
    "validation function need to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction using sliding window and linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#turn list of list to a list\n",
    "def flatten(list):\n",
    "    flattened_list=[]\n",
    "    for i in list:\n",
    "        for j in i:\n",
    "            flattened_list.append(j)\n",
    "    return flattened_list\n",
    "\n",
    "def prediction(signal,width,predLength):\n",
    "    #signal is what need to be predicted\n",
    "    #width is the width of sliding window, padding method is used\n",
    "    #predLength is how many steps to predict\n",
    "    signal=signal.tolist()\n",
    "    prediction=[]\n",
    "    for i in range(0,len(signal),predLength):\n",
    "        slice=[]\n",
    "        if i < width:\n",
    "            slice=slice+[0]*(width-i)\n",
    "            slice=slice+signal[0:i]\n",
    "        else:\n",
    "            slice=signal[i-width:i]\n",
    "            #predict value at location i\n",
    "        X=np.array(range(width)).reshape(-1,1)\n",
    "        y=np.array(slice).reshape(-1,1)\n",
    "        reg=LinearRegression().fit(X,y)\n",
    "        if i+predLength>=len(signal):\n",
    "            predLength=len(signal)-i\n",
    "        tmp=reg.predict(np.array(range(width,width+predLength)).reshape(-1,1))\n",
    "        prediction.extend(flatten(tmp))\n",
    "    return prediction\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#sliding window with width 5\n",
    "width=5\n",
    "predLength=2\n",
    "col=normalized_1.columns.tolist()\n",
    "col.remove('DATETIME')\n",
    "for i in col:\n",
    "    a=prediction(normalized_1[i][0:500],width,predLength)\n",
    "    #MSE\n",
    "    y_true=np.array(normalized_1[i][0:500])\n",
    "    y_predict=np.array(a)\n",
    "    mse=mean_squared_error(y_true,y_predict)\n",
    "    print(f'MSE of {i} and prediction with window width {width} and predLength {predLength} is: {mse}')\n",
    "    pd.Series(a).plot(figsize=(20,10),label='Predicted')\n",
    "    normalized_1[i][0:500].plot(figsize=(20,10))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting using ARMA\n",
    "\n",
    "Learn an autoregressive moving average model (see Wikipedia for an introduction if unfamiliar) for at least 5 individual sensors (pick them in a sensible way!). Most statistical packages (R, statsmodels in Python) contain standard algorithm for fitting these models from training data. Use autocorrelation plots in order to identify the order of the ARMA models. The parameters can be determined using Akaike’s Information Criterion (AIC) or another model selection method. Note that there exists a wide range of ARMA variants; you only have to use the basic model.Decide how to set the detection threshold sensibly. Study some of the anomalies detected anomalies. What kind of anomalies can you detect using ARMA models? Which sensors can be modeled effectively using ARMA?<br/>\n",
    "Steps:<br/>\n",
    "1.Pick 5 sensors<br/>\n",
    "2.autocorrelation plot http://people.duke.edu/~rnau/411arim3.htm  https://onlinecourses.science.psu.edu/stat510/lesson/3/3.1<br/>\n",
    "3.AIC to determine parameter (Box–Jenkins method)<br/>\n",
    "4.detection threshold<br/>\n",
    "5.detect anotomy and performance analysis<br/>\n",
    "https://otexts.com/fpp2/AR.html <br/>\n",
    "https://machinelearningmastery.com/difference-time-series-dataset-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Pick Sensors\n",
    "Attacks in the list will affect following sensors:<br/>\n",
    "1.L_T7,T7:low level<br/>\n",
    "2.like 1,PU10,PU11<br/>\n",
    "3.L_T1:low level, PU1/PU2:on T1:overflow<br/>\n",
    "4.like 3, pressure at pumps outlet<br/>\n",
    "5.PU7:reduce to 0.9, T4:low level<br/>\n",
    "6.PU7:reduced to 0.7 L_T4:drop concealed<br/>\n",
    "7.like 6, L_T1,PU1/PU2<br/>\n",
    "We choose L_T1,L_T7,F_PU1,F_PU7,F_PU10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Use autocorrelation plot to decide model order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized_2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use autocorrelation method to decide parameter \n",
    "\n",
    "# from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# autocorrelation_plot(normalized_1['F_PU1'][0:200])\n",
    "\n",
    "# %pip install statsmodels scipy\n",
    "\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def autocorrelation_plots(signal,name,lags=50):\n",
    "    series=signal[name]\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = sm.graphics.tsa.plot_acf(series.values.squeeze(), lags=lags, ax=ax1)\n",
    "    ax1.title.set_text(\"autocorrelation plot of \"+ name)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(series, lags=50, ax=ax2)\n",
    "    ax2.title.set_text(\"partial autocorrelation plot of \"+ name)\n",
    "    \n",
    "autocorrelation_plots(normalized_1,'F_PU1',300)\n",
    "\n",
    "# from pandas.plotting import autocorrelation_plot\n",
    "# fig = plt.figure(figsize=(12,8))\n",
    "# autocorrelation_plot(normalized_1['F_PU1'][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.F_PU1 is sinusoidal->AR(2)<br/>\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Determine parameter\n",
    "Time series signals are cyclic(time series with cyclic behavior is stationary), so no differencing is needed<br/>\n",
    "https://otexts.com/fpp2/arima-estimation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a model for estimation, it's inevitable to lose some information. AIC is used to measure the amount of information lost by using certain model. The smaller AIC is, the model is more close to real process. AIC looks after both underfitting and overfitting.<br/>\n",
    "$$AIC=2k-2ln(\\hat{L})$$\n",
    "k is the number of estimated parameters. \\hat{L} is the maximum value of the likelihood function for the model.<br/>\n",
    "BIC import the number of parameters, but in our assignment the numbers of parameters are similar.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import math\n",
    "\n",
    "def  comAIC(signal,name,rangeP:list,rangeQ:list):\n",
    "    series=signal[name]\n",
    "    min=math.inf\n",
    "    qstart=rangeQ[0]\n",
    "    qend=rangeQ[1]\n",
    "    pstart=rangeP[0]\n",
    "    pend=rangeP[1]\n",
    "    bestOrder=[]\n",
    "    coea=coem=[]\n",
    "    for i in range(pstart,pend+1,1):\n",
    "        for j in range(qstart,qend+1,1):\n",
    "                model = ARIMA(series, order=(i,0,j))\n",
    "                model_fit = model.fit(disp=0)\n",
    "                if model_fit.aic<min:\n",
    "                    min=model_fit.aic\n",
    "                    bestOrder=[i,0,j]\n",
    "                    coea=model_fit.arparams\n",
    "                    coem=model_fit.maparams\n",
    "                print(f'when p,d,q are {i,0,j} aic is, {model_fit.aic}')\n",
    "    print(f'min aic is {min} when p,d,q are {bestOrder}')\n",
    "    return bestOrder,min,coea,coem\n",
    "\n",
    "[order,aic,coea,coem]=comAIC(normalized_2,'F_PU1',[1,5],[1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.detect threshold\n",
    "trainging at every step is inefficient. So we only train once using training dataset 1. prediction is calculated based on formula. Need to notice that we've already standardized the time series so mean is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thresholdTraining(train,test,name,size):\n",
    "    attacks=test['ATT_FLAG']\n",
    "    attackindex=attacks.index[attacks == 1].tolist()\n",
    "    test=test[name].tolist()\n",
    "    #due to configuration constraint and the time-consuming trainging process, we only test 25 aic combinations\n",
    "    [order,aic,coea,coem]=comAIC(train,name,[1,size],[1,size])\n",
    "    #order is trained previously using training dataset 1\n",
    "    p=order[0]\n",
    "    q=order[-1]\n",
    "    size=max(len(coea),len(coem))\n",
    "    history=[x for x in test[-size:]]\n",
    "    predictions=list()\n",
    "    #predict time series\n",
    "    for t in range(len(test)):\n",
    "        history=history[-size:]\n",
    "        yhat=0\n",
    "        for j in range(len(coea)):\n",
    "            yhat+=coea[j]*history[j]\n",
    "        for j in range(len(coem)):\n",
    "            yhat+=coem[j]*history[j]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "    #MSE\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    print('Test MSE: %.3f' % error)\n",
    "    # plot\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(test,label='original')\n",
    "    plt.plot(predictions, color='red',label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    #train threshold\n",
    "    difference=[]\n",
    "    for i in range(len(test)):\n",
    "        difference.append(abs(test[i]-predictions[i]))\n",
    "    mean=np.mean(difference)\n",
    "    #range of threshold\n",
    "    difference=list(map(lambda x: abs(x-mean), difference))\n",
    "    maximum=max(difference)\n",
    "    minimum=min(difference)\n",
    "    threshold=0\n",
    "    bestPredictAttacks=None\n",
    "    for i in np.arange(minimum,maximum,0.01):\n",
    "        predictAttacks=[]\n",
    "        for t in range(len(test)):\n",
    "            if abs(predictions[t]-mean)>i:\n",
    "                predictAttacks.append(1)\n",
    "            else:\n",
    "                predictAttacks.append(0)\n",
    "        #f1 score\n",
    "        tp=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        best=0\n",
    "        f1=0\n",
    "        for t in range(len(test)):\n",
    "            if attacks[t]==1 and predictAttacks[t]==1:\n",
    "                tp+=1\n",
    "            if attacks[t]==1 and predictAttacks[t]==0:\n",
    "                fn+=1\n",
    "            if predicted[k]==0 and attacks[k]==1:\n",
    "                fp+=1        \n",
    "        if tp>0:\n",
    "            precision=tp/(tp+fp)\n",
    "            recall=tp/(tp+fn)\n",
    "            f1=2*(precision*recall)/(recall+precision)\n",
    "        if f1>best:\n",
    "            best=f1\n",
    "            threshold=i\n",
    "            bestPredictAttacks=predictAttacks     \n",
    "    #plot predicted attacks\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(attacks,color='yellow')\n",
    "    plt.plot(bestPredictAttacks,color='red')\n",
    "#     errorrate=best/np.sum(attacks)\n",
    "#     print(f'Error rate is {errorrate}')\n",
    "    print(f'threshold is {threshold}')\n",
    "    plt.show()\n",
    "    return order,coea,coem,threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholdTraining(normalized_1,normalized_2,'L_T1',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.detect anotomy and performance analysis<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(train1,train2,test,name,size):\n",
    "    order,coea,coem,threshold=thresholdTraining(train1,train2,name,size)\n",
    "    #first several samples are needed to predict following time series\n",
    "    test=test[name].tolist()\n",
    "    size=max(len(coea),len(coem))\n",
    "    history=test[0:size]\n",
    "    predictions=test[0:size]\n",
    "    #predict time series\n",
    "    for t in range(len(test)-size):\n",
    "        history=history[-size:]\n",
    "        yhat=0\n",
    "        for j in range(len(coea)):\n",
    "            yhat+=coea[j]*history[j]\n",
    "        for j in range(len(coem)):\n",
    "            yhat+=coem[j]*history[j]\n",
    "        predictions.append(yhat)\n",
    "        obs = test[t]\n",
    "        history.append(obs)\n",
    "    #MSE\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    print('Test MSE: %.3f' % error)\n",
    "    # plot predict time series\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(test,label='original')\n",
    "    plt.plot(predictions, color='red',label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    #plot predict anomalies\n",
    "    difference=[]\n",
    "    for i in range(len(test)):\n",
    "        difference.append(abs(test[i]-predictions[i]))\n",
    "    mean=np.mean(difference)\n",
    "    predictAttacks=[]\n",
    "    for i in range(len(predictions)):\n",
    "        if abs(predictions[i]-test[i])>threshold:\n",
    "            predictAttacks.append(1)\n",
    "        else:\n",
    "            predictAttacks.append(0)\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.plot(predictAttacks,color='red',label='predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "predict(normalized_1,normalized_2,normalized_t,'F_PU1',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Combine the 5 chosen signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "thresholdTraining(normalized_1,normalized_2,'L_T1',3)\n",
    "predict(normalized_1,normalized_2,normalized_t,'F_PU1',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete models task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the sensor data using any of the methods discussed in class. Explain why you choose this\n",
    "method and why the obtained discretization makes sense. Visualize the discretization.<br/>\n",
    "Apply any of the sequential data mining methods (N-grams, Sequence alignment with kNN, …) to\n",
    "sliding windows with a length of your choosing in order to find anomalies. Whenever an observed Ngram’s\n",
    "probability is too small, or the sequential data is too distant from any training sequence,\n",
    "raise an alarm. Set your thresholds sensibly. What kind of anomalies can you detect using the\n",
    "sequential model? Which sensors can be modeled effectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. discretize and visualize signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tslearn #you may need to install other module first\n",
    "from tslearn.piecewise import SymbolicAggregateApproximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discreteSignal(signal,name,n_sax,paa_width):\n",
    "    #n_sax: Number of SAX symbols to use\n",
    "    dataset=signal[name].copy()\n",
    "    n_sax_symbols = 8\n",
    "    n_paa_segments = math.ceil(len(dataset)/paa_width)\n",
    "    sax = SymbolicAggregateApproximation(n_segments=n_paa_segments, alphabet_size_avg=n_sax_symbols)\n",
    "    sax_dataset_inv = sax.inverse_transform(sax.fit_transform(dataset))\n",
    "    sax_signal=sax_dataset_inv[0].ravel().tolist()\n",
    "    return sax_signal\n",
    "\n",
    "def saxplot(signal,name,n_sax,paa_width,sax_signal):\n",
    "    dataset=signal[name].copy()\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(dataset[0:300], \"g\",label=name)\n",
    "    plt.plot(sax_signal[0:300], \"b\",label='sax')\n",
    "    plt.legend()\n",
    "    plt.title(f\"SAX {n_sax} symbols, paa width {paa_width}\" )\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_1_sax_LT1=discreteSignal(normalized_1,'L_T1',8,10)\n",
    "saxplot(normalized_1,'L_T1',8,10,normalized_1_sax_LT1)\n",
    "normalized_1_sax_LT7=discreteSignal(normalized_1,'L_T7',8,10)\n",
    "saxplot(normalized_1,'L_T7',8,10,normalized_1_sax_LT7)\n",
    "normalized_1_sax_F_PU1=discreteSignal(normalized_1,'F_PU1',8,10)\n",
    "saxplot(normalized_1,'F_PU1',8,10,normalized_1_sax_F_PU1)\n",
    "normalized_1_sax_F_PU7=discreteSignal(normalized_1,'F_PU7',8,10)\n",
    "saxplot(normalized_1,'F_PU7',8,10,normalized_1_sax_F_PU7)\n",
    "normalized_1_sax_F_PU10=discreteSignal(normalized_1,'F_PU10',8,10)\n",
    "saxplot(normalized_1,'F_PU10',8,10,normalized_1_sax_F_PU10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Apply KNN to sliding window\n",
    "I use distance-based KNN to predict anomaly<br/>\n",
    "~~Anomaly if distance to nearest neighbour n compared to distance from n to nearest neighbour is above threshold~~ : This will not work properly as it's quite common for neighbours have 0 distance in sax signal<br/>\n",
    "So instead, I use 'Anomaly if distance from other points is above threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSaxThreshold(signal,name,n_sax,paa_width,k,windowWidth):\n",
    "    attacks=signal['ATT_FLAG']\n",
    "    attackindex=attacks.index[attacks == 1].tolist()\n",
    "    #discretize signal\n",
    "    signal=discreteSignal(signal,name,n_sax,paa_width)\n",
    "    attacks=attacks.tolist()\n",
    "    minimum=10\n",
    "    maximum=0\n",
    "    #range of threshold\n",
    "    for i in attackindex:\n",
    "        if i>=windowWidth:\n",
    "            left=i-windowWidth\n",
    "        else:\n",
    "            left=0\n",
    "        tmp=signal[left:i]\n",
    "        tmp=list(map(lambda x:abs(x-signal[i]),tmp))\n",
    "        if maximum<max(tmp):\n",
    "            maximum=max(tmp)\n",
    "        if minimum>min(tmp):\n",
    "            minimum=min(tmp)\n",
    "    #train threshold\n",
    "    history=signal[0:windowWidth]\n",
    "    best=math.inf\n",
    "    for i in np.arange(minimum,maximum,0.01):\n",
    "        predictedAttacks=attacks[0:windowWidth]\n",
    "        for j in range(windowWidth,len(signal)):\n",
    "            distance=knnDistance(k,history,windowWidth,signal[j])\n",
    "            if distance>i:\n",
    "                predictedAttacks.append(1)\n",
    "            else:\n",
    "                predictedAttacks.append(0)\n",
    "            history.append(signal[j])\n",
    "        fp=0\n",
    "        for t in range(len(attacks)):\n",
    "            if attacks[t]==0 and predictedAttacks[t]==1:\n",
    "                fp+=1\n",
    "        if fp<best:    \n",
    "            best=fp\n",
    "            threshold=i\n",
    "            bestPredictAttacks=predictedAttacks     \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(signal,label='original',color='green',alpha=0.4)\n",
    "    plt.plot(attacks,label='original',color='red',alpha=0.4)\n",
    "    plt.plot(bestPredictAttacks,label='predicted',color='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return threshold\n",
    "            \n",
    "def predictSignal(signal,name,n_sax,paa_width,k,windowWidth,threshold):\n",
    "    signal=signal[name]\n",
    "#     signal=discreteSignal(signal,name,n_sax,paa_width)\n",
    "    history=signal[0:windowWidth]\n",
    "    predictedAttacks=[]\n",
    "    for i in range(windowWidth,len(signal)):\n",
    "        nextPoint=signal[i]\n",
    "        distance=knnDistance(k,history,windowWidth,nextPoint)\n",
    "        if distance>threshold:\n",
    "            predictedAttacks.append(1)\n",
    "        else:\n",
    "            predictedAttacks.append(0)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(signal,label='original',color='green')\n",
    "#     plt.plot(signal,label='original_sax',color='red')\n",
    "    plt.plot(predictedAttacks,label='predicted',color='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def knnDistance(k,previousSeries:list,windowWidth,nextPoint):\n",
    "    pointsInWindow=previousSeries[-windowWidth:]\n",
    "    distance=list(map(lambda x: abs(x-nextPoint), pointsInWindow))\n",
    "    distance.sort()\n",
    "    return sum(distance[0:k])/k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=predictSaxThreshold(normalized_2,'F_PU1',8,10,5,10)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSignal(normalized_t,'F_PU1',8,10,5,10,threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()\n",
    "pca_normlized_1=normalized_1.drop(columns={'ATT_FLAG','DATETIME'})\n",
    "pca_normlized_2=normalized_2.drop(columns={'ATT_FLAG','DATETIME'})\n",
    "pca_normlized_t=normalized_t.drop(columns={'DATETIME'})\n",
    "pca.fit(pca_normlized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/22126717/extracting-pca-components-with-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the components from transforming the original data.\n",
    "scores = pca.transform(pca_normlized_1)\n",
    "# Reconstruct from the 2 dimensional scores \n",
    "reconstruct = pca.inverse_transform(scores )\n",
    "#The residual is the amount not explained by the first two components\n",
    "residual=pow(pca_normlized_1-reconstruct,2)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is vulnearable to outliers, so I exclude 1% of all data points which are far from mean of the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total residual\n",
    "total_residual=[]\n",
    "for i in range(len(residual)):\n",
    "    total_residual.append(np.sum(residual.loc[i,:]))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(total_residual,label='original')\n",
    "#remove abnormal points\n",
    "mean=np.mean(total_residual)\n",
    "variance=np.std(total_residual)\n",
    "print(f'mean is {mean}')\n",
    "print(f'varivance is {variance}')\n",
    "removeIndex=[]\n",
    "print(f'previous length is {len(total_residual)}')\n",
    "threshold=0\n",
    "ratio=0\n",
    "res=None\n",
    "while ratio<0.99:\n",
    "    i=0\n",
    "    threshold+=0.1\n",
    "    res=total_residual.copy()\n",
    "    length=len(res)\n",
    "    while i != length:\n",
    "        if abs(res[i]-mean)>threshold*variance:\n",
    "            del res[i]\n",
    "            length-=1\n",
    "        else:\n",
    "            i+=1\n",
    "    ratio=len(res)/len(total_residual)\n",
    "\n",
    "print(f'after remove, the length is {len(res)}')\n",
    "print(f'threshold is {threshold}')\n",
    "\n",
    "i=j=0\n",
    "length=len(total_residual)\n",
    "index=[]\n",
    "while i != length:\n",
    "    if abs(total_residual[i]-mean)>threshold*variance:\n",
    "            del total_residual[i]\n",
    "            index.append(j)\n",
    "            length-=1\n",
    "    else:\n",
    "            i+=1\n",
    "    j+=1\n",
    "plt.plot(total_residual,label='after remove',color='red')\n",
    "plt.legend()\n",
    "\n",
    "print(f'index of removed samples are {index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove abnormal data\n",
    "cleanedData=pca_normlized_1.drop(pca_normlized_1.index[index])\n",
    "print(f'after remove length is {len(cleanedData)}')\n",
    "#refit pca\n",
    "pca.fit(cleanedData)\n",
    "scores = pca.transform(cleanedData)\n",
    "reconstruct = pca.inverse_transform(scores )\n",
    "refit_residual=cleanedData-reconstruct\n",
    "vr=pca.explained_variance_ratio_\n",
    "for i in range(len(vr)):\n",
    "    print(f'Percentage of variance explained by feature{i} is: {vr[i]}')\n",
    "#decide how many  components to keep\n",
    "sum=0\n",
    "numberComponent=0\n",
    "for i in vr:\n",
    "    if sum<0.99:\n",
    "        sum+=i\n",
    "        numberComponent+=1\n",
    "    else:\n",
    "        break\n",
    "print(f'number of principle component is {numberComponent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use training data 2 to calculate a proper threshold\n",
    "attacks=normalized_2['ATT_FLAG'].tolist()\n",
    "# attackindex=attacks.index[attacks == 1].tolist()\n",
    "pca = PCA(n_components=numberComponent)\n",
    "pca.fit(pca_normlized_2)\n",
    "scores = pca.transform(pca_normlized_2)\n",
    "reconstruct = pca.inverse_transform(scores )\n",
    "residual=np.sum(pow(pca_normlized_2-reconstruct,2),axis=1).tolist()\n",
    "threshold=0\n",
    "#range of threshold\n",
    "maximum=max(residual)\n",
    "minimum=min(residual)\n",
    "best=0\n",
    "predictedAttack=[]\n",
    "for i in np.arange(minimum,maximum,0.01):\n",
    "    predicted=[]\n",
    "    tp=0\n",
    "    fn=0\n",
    "    fp=0\n",
    "    for j in range(len(residual)):\n",
    "        if residual[j]>i:\n",
    "            predicted.append(1)\n",
    "        else:\n",
    "            predicted.append(0)\n",
    "    for k in range(len(residual)):\n",
    "        if predicted[k]==1 and attacks[k]==1:\n",
    "            tp+=1\n",
    "        if predicted[k]==1 and attacks[k]==0:\n",
    "            fn+=1\n",
    "        if predicted[k]==0 and attacks[k]==1:\n",
    "            fp+=1\n",
    "    if tp>0:\n",
    "        precision=tp/(tp+fp)\n",
    "        recall=tp/(tp+fn)\n",
    "        f1=2*(precision*recall)/(recall+precision)\n",
    "    if f1>best:\n",
    "        best=f1\n",
    "        threshold=i\n",
    "        predictedAttack=predicted     \n",
    "\n",
    "print(best)\n",
    "print(f'threshold is {threshold}')\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(attacks,label='original',color='red')\n",
    "plt.plot(predictedAttack,label='predicted',color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict on test data\n",
    "# attackindex=attacks.index[attacks == 1].tolist()\n",
    "pca = PCA(n_components=numberComponent)\n",
    "pca.fit(pca_normlized_t)\n",
    "scores = pca.transform(pca_normlized_t)\n",
    "reconstruct = pca.inverse_transform(scores )\n",
    "residual=np.sum(pow(pca_normlized_t-reconstruct,2),axis=1).tolist()\n",
    "prediction=[]\n",
    "for i in range(len(pca_normlized_t)):\n",
    "    if residual[i]>threshold:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(residual,label='residual',color='blue')\n",
    "plt.plot(prediction,label='predicted',color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
